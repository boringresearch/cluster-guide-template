[
["index.html", "UCSB Server Cluster User Guide Overview", " UCSB Server Cluster User Guide 2019-10-03 Overview The Center for Scientific Computing (CSC) at UCSB provides resources and support for research involving high performance computing (HPC), including multiple server clusters for storing and analyzing data. Find out more information at http://csc.cnsi.ucsb.edu/. This guide will help you access and use the UCSB server clusters effectively. Use cases for HPC include: big data parallel computing lengthy computation times restricted-use data For a broader introduction to HPC with clusters, see the HPC Carpentry’s lessons at https://hpc-carpentry.github.io/. The software available on the UCSB clusters includes R, Stata, Python, Julia, MATLAB, Git, and LaTeX. Additional information about available software can be found at http://csc.cnsi.ucsb.edu/docs/scientific-software. Please acknowledge the CSC in publications and presentations if you are using the clusters in your research. Also send the CSC the citations for your papers and presentations so they can include them in their publications list. "],
["accessing-the-server-clusters.html", "1 Accessing the server clusters 1.1 On campus 1.2 Off campus", " 1 Accessing the server clusters To access the server clusters, you must first request a user account. The request form can be found at http://csc.cnsi.ucsb.edu/acct. The job type selection should probably be Serial or Parallel, but select the option that best describes your needs. In the job type description you should describe any relevant information related to user groups, access to specific folders, or special software usage. If you need Stata, then you should request to be added to the econ group, which holds the license to use Stata. The system to be used should be Pod and Knot, which are the main Linux-based server clusters. Pod is the newest cluster and offers greater capabilities, but Knot is still adequate for most analysis needs and typically has less active users (i.e., a shorter job queue). 1.1 On campus Once your account has been created, the clusters can be accessed on campus via a terminal emulator using the ssh command or via a GUI SSH app, such as X2Go. On macOS or Linux the default terminal apps can be used. On Windows the PuTTY or X2Go apps can be used. Once a terminal is started, connect to a cluster by entering one of the commands ssh user@knot.cnsi.ucsb.edu or ssh user@pod.cnsi.ucsb.edu where user is your server username. You will then be prompted for your password. SSH keys can also be set up to skip password entry. Once connected, you will interact with the Linux server using the command line interface. The default working directory is your home folder /home/user. Note that this is the login node, but all analyses should be run on compute nodes. These analyses should be submitted as jobs (more here) to the queue, which allocates compute nodes among users. To disconnect from the cluster, enter exit. 1.2 Off campus The clusters can be accessed off campus using the UCSB Pulse Secure Connect VPN service for secure remote access. Instructions for installing the VPN app can be found at https://www.ets.ucsb.edu/services/campus-vpn/get-connected. Once the app is installed, open it and select connect for UCSB Remote Access. You will then be prompted to enter your UCSBnetID and the associated password. Once connected, indicated by a green checkmark, you can then access a cluster as you normally would on campus by using a terminal or GUI SSH app. "],
["managing-files.html", "2 Managing files 2.1 Navigation 2.2 Viewing, creating, and editing files 2.3 Permissions 2.4 Shortcuts", " 2 Managing files Both Pod and Knot allow for 4 TB of file storage per user and more can be requested if needed. File management on the server clusters involves commonly used Linux commands to navigate and modify files and folders. The generic setup is command [options] &lt;arguments&gt;. Most of the following commands have multiple options that are not covered here. For detailed documentation enter man command. Additionally, for a broader introduction to Linux commands see the Software Carpentry’s lessons at http://swcarpentry.github.io/shell-novice/. 2.1 Navigation pwd - present working directory cd dir - change directory (dir specifies directory path) cd - change to home directory (/home/user) cd .. - change to directory above current directory ls - list all files in current directory ls -a - list all files, including hidden files ls -l - list all files with permission information cp source destination - copy file mv source destination - move file mkdir name - make directory mkdir -m 770 name - make directory with permissions (770 or other number) rm file - remove file rmdir dir - remove empty directory (equivalent to rm -d dir) rm -r dir - remove non-empty directory shred -uz file - securely overwrite and delete file 2.2 Viewing, creating, and editing files file file.txt - view file type information stat file.txt - view file information more file.txt - view text file one screen at a time (to exit press q) less file.txt - view text file with scrolling (to exit press q) head file.csv - print first ten rows of text file tail file.csv - print last ten rows of text file head -1 file.csv - print column names if in first row of text file cut -d , -f 2,4-6 file.csv | less - view specific columns of .csv file by number position (e.g., 2,4-6) To view .csv files with pretty output, enter cat file.csv | sed -e 's/,,/, ,/g' | column -s, -t | less -#5 -N -S Note: This may not work well for all files. Use arrow keys to navigate and press q to exit. cat &gt; file.txt - create new text file (to save and exit press Ctrl+D) nano - create new text file using nano text editor nano file.txt - view and edit text file using nano text editor zip -r file.zip file1 folder1 - create compressed .zip file with recursion unzip file.zip - extract .zip file into working directory 2.3 Permissions Sometimes file and folder permissions need to be modified, such as to restrict access to files. On Linux, read, write, and execute permissions are represented by octal notation and applied to the file owner, groups, and all other users. # Permission rwx 7 read, write, and execute rwx 6 read and write rw- 5 read and execute r-x 4 read only r– 3 write and execute -wx 2 write only -w- 1 execute only –x 0 none — Common permissions include -rwxrwx--- = 770 - owner and group can do everything, but others can do nothing -rwxr-x--- = 750 - owner can do everything, group can read and execute only, but others can do nothing -rwx------ = 700 - owner can do everything, but group and others can do nothing To change permissions enter chmod 770 dir - change permissions for file or directory chmod -R 770 dir - change permissions recursively for directory chgrp group file - change group ownership for file or directory 2.4 Shortcuts Ctrl+A - move to beginning of line Ctrl+E - move to end of line Ctrl+U - clear line from cursor Ctrl+C - cancel command * - wildcard completion Tab key - autocompletion Up and down arrow keys - cycle through command history "],
["transferring-files.html", "3 Transferring files 3.1 Using scp to transfer files 3.2 Using rsync to transfer files", " 3 Transferring files To transfer files between a server and your local computer, use scp or rsync commands in the terminal or use a SFTP GUI app. On macOS, Cyberduck and FileZilla are good options. On Windows, WinSCP, Cyberduck, and FileZilla are good options. On Linux, FileZilla is a good option. Another option is to use Globus Online, a web app focused on large file transfers that allows pauses and breaks without loss of data. 3.1 Using scp to transfer files The scp command provides secure copying of files. For more detailed documentation enter the command man scp. The generic setup for an scp command is scp [options] &lt;source&gt; &lt;destination&gt; To copy a single file from a local computer to the Pod cluster, the scp command is scp /path/to/file.txt user@pod.cnsi.ucsb.edu:/home/user To copy an entire directory from a local computer to the Pod server cluster, the typical scp command is scp -r /path/to/project user@pod.cnsi.ucsb.edu:/home/user where the -r option indicates that files should be transferred recursively, such as subdirectories. 3.2 Using rsync to transfer files The rsync command line tool provides fast, incremental file transfer. The primary use case for rsync is to sync two folders, such as a synced backup folder. Compared to scp, rsync only transfers modified or new files and may also use partial transfers. As a result, rsync is typically faster than scp and SFTP, depending on the options used. For more detailed documentation enter the command man rsync or see https://rsync.samba.org/. On macOS and Linux, rsync is typically pre-installed. On Windows, you will need to install and use rsync via Cygwin. The generic setup for an rsync command is rsync [options] &lt;source&gt; &lt;destination&gt; To transfer an entire directory from a local computer to the Pod cluster, a typical rsync command with commonly used options is rsync -avzP -e ssh /home/user/project/ user@pod.cnsi.ucsb.edu:/home/user/project This example represents a push transfer. A pull transfer could also be completed by simply switching the source and destination. When transferring entire directories, a forward slash on the source matters but never for the destination. In this case, the ‘project’ folder has a forward slash and all its contents will be replicated exactly in the destination ‘project’ folder, copying all contents except the top ‘project’ folder. Without the forward slash on the source there would be another ‘project’ folder within the ‘project’ folder at the destination. The rsync options can be specified in short or long forms. In this use case, the -a or --archive option completely replicates all folders and files, including recursively through all subdirectories while preserving symbolic links, permissions, and ownership. The -v or --verbose option increases the amount of information that is logged. The -z or --compress option compresses files during transit to reduce transfer time. The -P or --partial --progress option enables partially transferred files to be kept in case of a break or pause and also displays the progress of individual file transfers. The -e ssh option instructs rysnc to transfer files via SSH, used when transferring to or from a server. Many other options exist, but these are the primary ones used. Additionally, add the -n or --dry-run option to test what an rsync command will do without actually transferring files. After the command is submitted, you will be prompted for a password if transferring to or from a server. If you use rsync frequently, then you can also set up SSH keys in order to skip password entry. For large transfers, sometimes the process needs to be paused and resumed, or sometimes the transfer can be interrupted due to a server, network, or power outage. The -P or --partial option should always be used to preserve the files or parts of files that have already been transferred and to avoid having to start over. To pause a transfer, use Ctrl+C. To resume, resubmit the same rsync command with the --append option added, which will restart the transfer where it left off. When syncing to a backup folder, add the --delete option to delete files and folders in the destination that have been deleted in the source. "],
["submitting-jobs.html", "4 Submitting jobs 4.1 Knot 4.2 Pod", " 4 Submitting jobs The server clusters are shared resources among researchers, and a job queue process is used to manage and allocate resources among users. A job is simply a set of instructions that includes requests for resources and the specific set of commands (typically scripts) to be executed, such as commands for transforming or analyzing data. When a user submits a job to the server for execution it enters the queue and is scheduled on a specific compute node for a specific time. 4.1 Knot The Knot cluster has 112 regular compute nodes with 12 cores per node and either 48 GB or 64 GB of RAM per node. There are also 4 ‘fat’ nodes with either 512 GB or 1 TB of RAM and 6 GPU nodes. Knot uses TORQUE PBS to schedule jobs. To submit a job, first create a new .pbs file with the command nano submit.pbs The typical structure of a .pbs file for a serial job using R is #!/bin/bash #PBS -l nodes=1:ppn=1 #PBS -l walltime=2:00:00 #PBS -V cd $PBS_O_WORKDIR Rscript script.R where 1 node with 1 processor is requested with a 2-hour timeframe for computation. The walltime option can be excluded if the computation time is unknown. The default walltime is 75 hours. The cd $PBS_O_WORKDIR changes the working directory to where the .job file is located. The Rscript script.R line executes the commands in the specified R script. The filepath for the script should be relative to where the .pbs file is located or an absolute path can also be used. This line would change if using different software. There are also many other #PBS options that can be included. Parallel jobs require different specifications, such as requesting more than 1 node and using mpirun commands. Also be sure to include a blank line at the end of the file. To submit a job, use the command qsub submit.pbs For jobs that require less than one hour or are used for testing and debugging purposes, use the short queue to minimize waiting time with the command qsub -q short submit.pbs For jobs that require large memory, use one of the commands # 256 GB/node qsub -q largemem submit.pbs # or 512 GB/node qsub -q xlargemem submit.pbs The qsub command will return a job number. To check the status of a job, use the command qstat &lt;job number&gt; # or qstat -u $USER To cancel or delete a job, use the command qdel &lt;job number&gt; The outputs of the analysis in the script will be returned in the same folder as the .pbs file that was submitted, typically with the filename structure submit.pbs.[job number] unless otherwise specified. 4.2 Pod The Pod cluster is the newest on campus and offers the most compute resources. There are 64 regular compute nodes with 40 processors each and 192 GB of RAM per node. There are also 4 ‘fat’ nodes with 1 TB of RAM per node and 3 GPU nodes. Pod uses SLURM to schedule jobs. To submit a job, first create a new .job file with the command nano submit.job The typical structure of a .job file for a serial job using R is #!/bin/bash -l #SBATCH --nodes=1 --ntasks-per-node=1 cd $SLURM_SUBMIT_DIR module load R Rscript script.R where 1 node with 1 processor is requested. The cd $PBS_O_WORKDIR changes the working directory to where the .job file is located. The module load R line loads R, and then the Rscript script.R line executes the commands in the specified R script. The filepath for the script should be relative to where the .job file is located or an absolute path can also be used. These last two lines would change if using different software. There are also many other #SBATCH options that can be included. The default walltime for computation is 32 hours. Parallel jobs require different specifications, such as requesting more than 1 node and using mpirun commands. Also be sure to include a blank line at the end of the file. To submit a job, use the command sbatch submit.job For jobs that require less than one hour or are used for testing and debugging purposes, you can use the short queue to minimize waiting time with the command sbatch -p short submit.job For jobs that require large memory, use the command # 1 TB/node sbatch -p largemem submit.job The sbatch command will return a job number. To check the status of a job, use one of the commands showq &lt;job number&gt; # or showq | grep $USER # or squeue -u $USER To cancel or delete a job, use the command scancel -i &lt;job number&gt; The outputs of the analysis in the script will be returned in the same folder as the .job file that was submitted, typically with the filename structure slurm-[jobnumber].out unless otherwise specified. "],
["using-r.html", "5 Using R 5.1 Loading R 5.2 Versions 5.3 Packages 5.4 Example job files", " 5 Using R R is available on both Knot and Pod, including different versions of R. 5.1 Loading R To load R, enter one of the commands # Knot R or # Pod load module R R To exit, enter q(). Note that the RStudio IDE is not available for use on the clusters. Remember: Most analyses should be performed on compute nodes by submitting jobs. The login node should only be used for simple analyses, testing, or debugging. 5.2 Versions On Pod, the available versions of R include R/3.2.2 R/3.4.4 R/3.5.1-multith R/3.5.1 The default is the latest version. To load a specific version, for example, enter load module R/3.4.4. On Knot, the available version of R is 3.2.2. A different version of R can be installed inside your home directory. For more info see http://csc.cnsi.ucsb.edu/docs/using-r-knot-braid-and-pod. 5.3 Packages To see a list of already installed R packages and their versions, use the R command installed.packages(). Additional packages can be installed using the R command install.packages(&quot;package&quot;) and should be stored inside your home folder. When using this command you may be prompted to select a CRAN mirror from which to download; select a USA (CA) mirror with an HTTPS connection for a fast and secure download. R should automatically add the home-based library to .libPaths() (enter to confirm), which allows it to detect packages installed inside your home folder. 5.4 Example job files 5.4.1 Knot #!/bin/bash #PBS -l nodes=1:ppn=1 #PBS -m be #PBS -M user@ucsb.edu cd $PBS_O_WORKDIR Rscript script.R 5.4.2 Pod #!/bin/bash -l #SBATCH --nodes=1 --ntasks-per-node=1 #SBATCH --mail-type=ALL #SBATCH --mail-user=user@ucsb.edu cd $SLURM_SUBMIT_DIR module load R Rscript script.R "],
["using-stata.html", "6 Using Stata 6.1 Loading Stata 6.2 User-written packages 6.3 Example job files", " 6 Using Stata On both Pod and Knot, Stata/MP 15 is available with up to 24 cores. In order to use Stata, you will need to be added to the econ group (contact Fuzzy Rogers - fuz(at)ucsb.edu). 6.1 Loading Stata To load Stata, enter the command /sw/stata/stata-mp To exit, enter exit. Specific versions of Stata can be used by entering the version # command. To load the Stata GUI, connect to a cluster via an XWindows environment and then enter the command /sw/stata/xstata-mp The GUI can be useful for viewing graphics, but will be relatively slow. Remember: Most analyses should be performed on compute nodes by submitting jobs. The login node should only be used for simple analyses, testing, or debugging. 6.2 User-written packages User-written Stata packages can be installed by using the Stata command ssc install package and should be stored inside your home directory. Point to the packages in do-files by using the Stata command sysdir set PLUS dir where dir should be the path to the ado/plus directory inside your home folder. 6.3 Example job files Note that Stata has a set processors # command that should be included in do-files and match the requested resources in job files (up to 24 processors). 6.3.1 Knot #!/bin/bash #PBS -l nodes=1:ppn=12 #PBS -l walltime=2:00:00 #PBS -V #PBS -m be #PBS -M user@ucsb.edu cd $PBS_O_WORKDIR/ /sw/stata/stata-mp -b do script.do 6.3.2 Pod #!/bin/bash #SBATCH --nodes=1 --ntasks-per-node=24 #SBATCH --mail-type=ALL #SBATCH --mail-user=user@ucsb.edu cd $SLURM_SUBMIT_DIR /sw/stata/stata-mp -b do script.do "],
["using-software-containers.html", "7 Using software containers", " 7 Using software containers Software containers can be used on the server clusters via Singularity. Containers enable fully reproducible research by packaging a computing environment and the necessary software packages and applications as a self-contained image file that can be easily transferred to and run on other systems. Containers are also useful for collaborating across different institutions and computing infrastructures. Find out more information at http://csc.cnsi.ucsb.edu/docs/containers. "]
]
